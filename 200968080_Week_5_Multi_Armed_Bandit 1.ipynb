{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["- The objective of the agent is to maximize its total reward over a certain period of time, but it does not know the true distributions of the different actions in advance. \n","- The agent must explore by trying different actions and collecting data on their payoffs, while also exploiting the knowledge it has gained so far to choose the best action based on the available information.\n","- This trade-off between exploration and exploitation is the main challenge of the MAB problem."],"metadata":{"id":"dPzuX0hcIxSY"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SjlWvDu18hw2","outputId":"936c918c-7b81-438e-86a2-0f4fac4f5858","executionInfo":{"status":"ok","timestamp":1680094515219,"user_tz":-330,"elapsed":21239,"user":{"displayName":"Kartheek T","userId":"13688682006331270789"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tf-agents\n","  Downloading tf_agents-0.16.0-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from tf-agents) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.9/dist-packages (from tf-agents) (2.2.1)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.9/dist-packages (from tf-agents) (3.19.6)\n","Requirement already satisfied: tensorflow-probability~=0.19.0 in /usr/local/lib/python3.9/dist-packages (from tf-agents) (0.19.0)\n","Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.9/dist-packages (from tf-agents) (1.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from tf-agents) (4.5.0)\n","Collecting pygame==2.1.3\n","  Downloading pygame-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from tf-agents) (0.5.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from tf-agents) (8.4.0)\n","Collecting gym<=0.23.0,>=0.17.0\n","  Downloading gym-0.23.0.tar.gz (624 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 KB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.9/dist-packages (from tf-agents) (1.22.4)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.9/dist-packages (from tf-agents) (1.15.0)\n","Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.9/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (6.1.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability~=0.19.0->tf-agents) (0.1.8)\n","Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability~=0.19.0->tf-agents) (0.4.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability~=0.19.0->tf-agents) (4.4.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.15.0)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697660 sha256=51ae89d6e24facbee1d219bf2ac609e13062357a732e8cbc6d3c05da0c6e31e1\n","  Stored in directory: /root/.cache/pip/wheels/96/b9/bb/994c1324b65e39dd1cd7b8ba92e5fb766dd77980929414a866\n","Successfully built gym\n","Installing collected packages: pygame, gym, tf-agents\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.3.0\n","    Uninstalling pygame-2.3.0:\n","      Successfully uninstalled pygame-2.3.0\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","Successfully installed gym-0.23.0 pygame-2.1.3 tf-agents-0.16.0\n"]}],"source":["pip install tf-agents"]},{"cell_type":"markdown","source":["### Exercise-1\n"],"metadata":{"id":"FIZbMVwSqsyW"}},{"cell_type":"markdown","source":["#### Creating a Environment for which the observation is a random integer between -5 and 5, there are 3 possible actions (0, 1, 2), and the reward is the product of the action and the observation.\n","\n","\n","\n"],"metadata":{"id":"zouwXij2-yeu"}},{"cell_type":"code","source":["from tf_agents.environments import py_environment\n","from tf_agents.specs import array_spec\n","from tf_agents.trajectories import time_step as ts\n","import numpy as np\n","import tensorflow as tf\n","from tf_agents.trajectories import policy_step\n","from tf_agents.agents import TFAgent\n","from tf_agents.policies import random_py_policy, tf_policy\n","from tf_agents.utils import common\n","from tf_agents.drivers import dynamic_episode_driver\n","\n","\n","\n","class MyEnvironment(py_environment.PyEnvironment):\n","\n","  def __init__(self):\n","    self._observation_spec = array_spec.BoundedArraySpec(\n","        shape=(), dtype=np.int32, minimum=-5, maximum=5, name='observation')    # observation range from -5 to +5\n","    self._action_spec = array_spec.BoundedArraySpec(\n","        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')          # action range from 0 to 2 \n","    self._episode_ended = False\n","    self._observation = None\n","    self._reward = None\n","\n","  def action_spec(self):\n","    return self._action_spec\n","\n","  def observation_spec(self):\n","    return self._observation_spec\n","\n","  def _reset(self):\n","    self._observation = np.random.randint(-5, 6)\n","    self._episode_ended = False\n","    return ts.restart(np.array(self._observation, dtype=np.int32))\n","\n","  def _step(self, action):\n","    if self._episode_ended:\n","      return self.reset()\n","\n","    if action < 0 or action > 2:      # termination step\n","      return self.reset()\n","\n","    self._reward = self._observation * action\n","    self._episode_ended = True\n","    return ts.termination(np.array(self._observation, dtype=np.int32), reward=self._reward)\n","\n"],"metadata":{"id":"V9NYdM5p-MfH","executionInfo":{"status":"ok","timestamp":1680094770783,"user_tz":-330,"elapsed":3606,"user":{"displayName":"Kartheek T","userId":"13688682006331270789"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["#### Define an optimal policy manually. The action only depends on the sign of the observation, 0 when is negative and 2 when is positive."],"metadata":{"id":"7wjKH1Wa-2Qt"}},{"cell_type":"code","source":["class OptimalPolicy(tf.Module):\n","    def __init__(self):\n","        pass\n","\n","    def action(self, observation):\n","        action = 0 if observation < 0 else 2\n","        return policy_step.PolicyStep(action=action, state=())"],"metadata":{"id":"KHEXfPzp-MXC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Request for 50 observations from the environment, compute and print the total reward."],"metadata":{"id":"dCtxSh6G_PP-"}},{"cell_type":"code","source":["env = MyEnvironment()\n","policy = OptimalPolicy()\n","\n","total_reward = 0\n","for _ in range(50):\n","  time_step = env.reset()\n","  action_step = policy.action(time_step.observation)\n","  next_time_step = env.step(action_step.action)\n","  total_reward += next_time_step.reward\n","\n","print(\"Total reward:\", total_reward)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ZKnTq5J--GN","outputId":"235a57e1-6d46-414d-b6bd-cc0e85de72b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total reward: 136.0\n"]}]},{"cell_type":"markdown","source":["### Exercise-2"],"metadata":{"id":"DCYtiEGaqxpS"}},{"cell_type":"markdown","source":["#### Define an environment will either always give reward = observation * action or reward = -observation * action. This will be decided when the environment is initialized.\n"],"metadata":{"id":"hRMCw8Q2_e-Z"}},{"cell_type":"code","source":["class MyEnvironment(py_environment.PyEnvironment):\n","    def __init__(self):\n","        self._action_spec = array_spec.BoundedArraySpec(\n","            shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n","        self._observation_spec = array_spec.BoundedArraySpec(\n","            shape=(), dtype=np.int32, minimum=-5, maximum=5, name='observation')\n","        self._reward_spec = array_spec.BoundedArraySpec(\n","            shape=(), dtype=np.float32, minimum=-25, maximum=25, name='reward')\n","        self._observation = None\n","        self._reward = None\n","        self._flip = np.random.choice([True, False])\n","        super(MyEnvironment, self).__init__()\n","\n","    def action_spec(self):\n","        return self._action_spec\n","\n","    def observation_spec(self):\n","        return self._observation_spec\n","\n","    def _reset(self):\n","        self._observation = np.random.randint(low=-5, high=6)\n","        self._reward = self._observation * np.random.choice([-1, 1])\n","        return time_step.restart(observation=self._observation)\n","\n","    def _step(self, action):\n","        if action < 0 or action > 2:\n","            return self.reset()\n","        if self._flip:\n","            self._reward = self._observation * action\n","        else:\n","            self._reward = -self._observation * action\n","        self._flip = not self._flip\n","        return time_step.transition(\n","            observation=self._observation, reward=self._reward, discount=1.0)\n"],"metadata":{"id":"Zwz7_OIs_j1I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Define a policy that detects the behavior of the underlying environment. \n","- There are three situations that the policy needs to handle\n","\n","  - The agent has not detected know yet which version of the environment \n","  is running.\n","  - The agent detected that the original version of the environment is \n","  running.\n","  - The agent detected that the flipped version of the environment is \n","  running."],"metadata":{"id":"KJ4ezshEBBOg"}},{"cell_type":"code","source":["class DetectPolicy(tf.Module):\n","    def __init__(self, num_actions):\n","        self.num_actions = num_actions\n","        self.counts = np.zeros((num_actions,))\n","        self.values = np.zeros((num_actions,))\n","        self.total_counts = 0\n","        self.last_action = None\n","        self.detected_environment = None\n","    \n","    def __call__(self, time_step):\n","        if self.last_action is not None:\n","            self.update(time_step.reward)\n","        \n","        if self.detected_environment is None:\n","            action = np.random.randint(self.num_actions)\n","        elif self.detected_environment == \"original\":\n","            action = np.argmax(self.values)\n","        elif self.detected_environment == \"flipped\":\n","            action = np.argmin(self.values)\n","        \n","        self.last_action = action\n","        return policy_step.PolicyStep(action=action, state=())\n","    \n","    def update(self, reward):\n","        self.counts[self.last_action] += 1\n","        self.total_counts += 1\n","        alpha = 1.0 / self.counts[self.last_action]\n","        self.values[self.last_action] += alpha * (reward - self.values[self.last_action])\n","        \n","        if self.detected_environment is None and self.total_counts >= self.num_actions:\n","            if self.values[0] > self.values[1]:\n","                self.detected_environment = \"original\"\n","            else:\n","                self.detected_environment = \"flipped\"\n"],"metadata":{"id":"nHoJWtIwA9KJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Define the agent that detects the sign of the environment and sets the policy appropriately"],"metadata":{"id":"PlmTc3geBdne"}},{"cell_type":"code","source":["class SignDetectionAgent(TFAgent):\n","    def __init__(self, environment, policy):\n","        self._environment = environment\n","        self._policy = policy\n","        super().__init__(\n","            time_step_spec=environment.time_step_spec(),\n","            action_spec=environment.action_spec(),\n","            policy=policy,\n","            collect_policy=policy,\n","            train_sequence_length=2,\n","            num_outer_dims=1)\n","        \n","    def _initialize(self):\n","        # Run an episode to learn the sign of the environment\n","        self._environment.reset()\n","        time_step = self._environment.step(self._environment.action_spec().sample())\n","        while not time_step.is_last():\n","            time_step = self._environment.step(self._environment.action_spec().sample())\n","        observation = time_step.observation\n","        \n","        # Set the policy based on the sign of the observation\n","        if observation < 0:\n","            self._policy = ZeroPolicy()\n","        else:\n","            self._policy = DoublePolicy()\n","        \n","        # Update the policy for the agent\n","        self._set_policy(self._policy)\n","        \n","    def train(self, experience, **kwargs):\n","        if not self._policy:\n","            self._initialize()\n","        return super().train(experience, **kwargs)\n","    \n","class ZeroPolicy(tf_policy.TFPolicy):\n","    def __init__(self):\n","        super().__init__(\n","            time_step_spec=ts.time_step_spec(tensor_spec.TensorSpec([], tf.int32)),\n","            action_spec=tensor_spec.BoundedTensorSpec([], tf.int32, 0, 2),\n","            policy_state_spec=(),\n","            info_spec=())\n","        \n","    def _variables(self):\n","        return []\n","\n","    def _distribution(self, time_step):\n","        action = tf.constant(0, dtype=tf.int32)\n","        return tfp.distributions.Deterministic(action)\n","    \n","class DoublePolicy(tf_policy.TFPolicy):\n","    def __init__(self):\n","        super().__init__(\n","            time_step_spec=ts.time_step_spec(tensor_spec.TensorSpec([], tf.int32)),\n","            action_spec=tensor_spec.BoundedTensorSpec([], tf.int32, 0, 2),\n","            policy_state_spec=(),\n","            info_spec=())\n","        \n","    def _variables(self):\n","        return []\n","\n","    def _distribution(self, time_step):\n","        action = tf.constant(2, dtype=tf.int32)\n","        return tfp.distributions.Deterministic(action)\n"],"metadata":{"id":"8utKRRSEBuqK"},"execution_count":null,"outputs":[]}]}