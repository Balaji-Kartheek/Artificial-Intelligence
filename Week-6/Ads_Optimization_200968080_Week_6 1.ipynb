{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"iGFqtYXm0L0m","executionInfo":{"status":"ok","timestamp":1680110854615,"user_tz":-330,"elapsed":1904,"user":{"displayName":"Kartheek T","userId":"13688682006331270789"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"markdown","source":["### A. Write down the MAB agent problem formulation in your own words."],"metadata":{"id":"WRP4hCwjFsUj"}},{"cell_type":"markdown","source":["- The MAB agent needs to decide which ad to display at each time step to maximize the total number of clicks received over a large number of time steps.\n","- The agent's objective is to maximize the total reward obtained by displaying ads to users.\n","- There are multiple ads to choose, each with an unknown click-through distribution that determines the probability of user clicking on the ad.\n","- The agent must balance the tradeoff between exploration and exploitation to maximize the total reward obtained, taking into account the cost of displaying ads that are less likely to be clicked.\n","- Various Algorithms are used to solve MAB problem\n","  - eplison-greedy\n","  - Upper-Confidence-Bound method\n"],"metadata":{"id":"zs9De-IpF0Su"}},{"cell_type":"markdown","source":["### B.Compute the total rewards after 2000-time steps using the ε-greedy action. for \n","- ε=0.01, \n","- ε= 0.3"],"metadata":{"id":"exssHhZI1qZz"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V-QMFQNUjD9k","executionInfo":{"status":"ok","timestamp":1680110890212,"user_tz":-330,"elapsed":35607,"user":{"displayName":"Kartheek T","userId":"13688682006331270789"}},"outputId":"51a216ce-322f-44b3-912f-ddc17e1127c6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","\n","df = pd.read_csv(\"/content/drive/MyDrive/Artificial_Intelligence/Week-6/Ads_Optimisation.csv\")\n","num_ads = 10\n","num_rounds = 2000\n","\n","ads_selected = []\n","total_rewards = 0\n","num_selections = np.zeros(num_ads)\n","sum_rewards = np.zeros(num_ads)\n","\n","epsilons = [0.01, 0.3]\n","\n","for eps in epsilons:\n","    for n in range(num_rounds):\n","\n","        if np.random.random() > eps:   \n","            ad = np.argmax(sum_rewards / (num_selections + 1))    # random ad reward selection\n","        else:\n","            ad = np.random.randint(num_ads)     # random selection\n","\n","        ads_selected.append(ad)\n","        reward = df.values[n, ad]\n","        total_rewards += reward\n","        num_selections[ad] += 1\n","        sum_rewards[ad] += reward\n","        if n % 100 == 0:\n","          print(f\"Selected ad for round {n}: {ad} (Epsilon={eps})\")\n","    \n","    print(\"------------------------------------------------------------------\")\n","    print(f\"Total reward for epsilon={eps}: {total_rewards}\")\n","    import statistics\n","    most_frequent = statistics.mode(ads_selected)\n","    print(most_frequent)\n","    max_val = max(ads_selected)\n","    print(f\"Ad selected in epsilon method is:= {max_val}\")\n","    print(\"_________________________________________________________________________________________________________________________________\")\n","    \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mA8ZviBo0WpV","outputId":"ada59287-50c5-4354-8ee4-f612c4f7d37d","executionInfo":{"status":"ok","timestamp":1680110890212,"user_tz":-330,"elapsed":15,"user":{"displayName":"Kartheek T","userId":"13688682006331270789"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Selected ad for round 0: 0 (Epsilon=0.01)\n","Selected ad for round 100: 0 (Epsilon=0.01)\n","Selected ad for round 200: 0 (Epsilon=0.01)\n","Selected ad for round 300: 0 (Epsilon=0.01)\n","Selected ad for round 400: 0 (Epsilon=0.01)\n","Selected ad for round 500: 0 (Epsilon=0.01)\n","Selected ad for round 600: 0 (Epsilon=0.01)\n","Selected ad for round 700: 0 (Epsilon=0.01)\n","Selected ad for round 800: 0 (Epsilon=0.01)\n","Selected ad for round 900: 0 (Epsilon=0.01)\n","Selected ad for round 1000: 0 (Epsilon=0.01)\n","Selected ad for round 1100: 0 (Epsilon=0.01)\n","Selected ad for round 1200: 0 (Epsilon=0.01)\n","Selected ad for round 1300: 0 (Epsilon=0.01)\n","Selected ad for round 1400: 0 (Epsilon=0.01)\n","Selected ad for round 1500: 0 (Epsilon=0.01)\n","Selected ad for round 1600: 0 (Epsilon=0.01)\n","Selected ad for round 1700: 0 (Epsilon=0.01)\n","Selected ad for round 1800: 0 (Epsilon=0.01)\n","Selected ad for round 1900: 0 (Epsilon=0.01)\n","------------------------------------------------------------------\n","Total reward for epsilon=0.01: 337\n","0\n","Ad selected in epsilon method is:= 8\n","_________________________________________________________________________________________________________________________________\n","Selected ad for round 0: 0 (Epsilon=0.3)\n","Selected ad for round 100: 3 (Epsilon=0.3)\n","Selected ad for round 200: 0 (Epsilon=0.3)\n","Selected ad for round 300: 0 (Epsilon=0.3)\n","Selected ad for round 400: 4 (Epsilon=0.3)\n","Selected ad for round 500: 4 (Epsilon=0.3)\n","Selected ad for round 600: 4 (Epsilon=0.3)\n","Selected ad for round 700: 6 (Epsilon=0.3)\n","Selected ad for round 800: 4 (Epsilon=0.3)\n","Selected ad for round 900: 0 (Epsilon=0.3)\n","Selected ad for round 1000: 7 (Epsilon=0.3)\n","Selected ad for round 1100: 4 (Epsilon=0.3)\n","Selected ad for round 1200: 4 (Epsilon=0.3)\n","Selected ad for round 1300: 5 (Epsilon=0.3)\n","Selected ad for round 1400: 5 (Epsilon=0.3)\n","Selected ad for round 1500: 4 (Epsilon=0.3)\n","Selected ad for round 1600: 4 (Epsilon=0.3)\n","Selected ad for round 1700: 1 (Epsilon=0.3)\n","Selected ad for round 1800: 4 (Epsilon=0.3)\n","Selected ad for round 1900: 4 (Epsilon=0.3)\n","------------------------------------------------------------------\n","Total reward for epsilon=0.3: 741\n","0\n","Ad selected in epsilon method is:= 9\n","_________________________________________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["### C. Compute the total rewards after 2000-time steps using the Upper-Confidence-Bound action method for c= 1.5"],"metadata":{"id":"3D2W3Xqq1vSM"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","df = pd.read_csv(\"/content/drive/MyDrive/Artificial_Intelligence/Week-6/Ads_Optimisation.csv\")\n","num_ads = 10\n","num_rounds = 2000\n","\n","ads_selected = []\n","total_rewards = 0\n","num_selections = np.zeros(num_ads)\n","sum_rewards = np.zeros(num_ads)\n","\n","# exploration parameter c\n","c = 1.5\n","\n","for n in range(num_rounds):\n","    # select ad using UCB action method\n","    ad = 0\n","    max_upper_bound = 0\n","    for i in range(num_ads):\n","        if num_selections[i] > 0:\n","            average_reward = sum_rewards[i] / num_selections[i]\n","            delta_i = np.sqrt(2 * np.log(n+1) / num_selections[i])\n","            upper_bound = average_reward + c * delta_i\n","        else:\n","            upper_bound = 1e400\n","        if upper_bound > max_upper_bound:\n","            max_upper_bound = upper_bound\n","            ad = i\n","\n","    ads_selected.append(ad)\n","\n","    reward = df.values[n, ad]\n","    total_rewards += reward\n","    num_selections[ad] += 1\n","    sum_rewards[ad] += reward\n","\n","print(f\"Total reward for c={c}: {total_rewards}\")\n","import statistics\n","\n","most_frequent = statistics.mode(ads_selected)\n","print(most_frequent)\n","\n","print(f\"Ad selected in UCB method is:= {max_val}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZWVQUFDi1vF8","outputId":"8c9f359a-ec95-403a-c8d4-64dd85f10792","executionInfo":{"status":"ok","timestamp":1680111106666,"user_tz":-330,"elapsed":432,"user":{"displayName":"Kartheek T","userId":"13688682006331270789"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Total reward for c=1.5: 290\n","4\n","Ad selected in UCB method is:= 9\n"]}]},{"cell_type":"markdown","source":["### D. For all approaches, explain how the action value estimated compares to the optimal action"],"metadata":{"id":"vPMvKyF71mrT"}},{"cell_type":"markdown","source":["#### For the epsilon-greedy method:\n","\n","- The estimated action values improve as the number of rounds increases\n","- As the value of epsilon decreases, the algorithm becomes more greedy and relies more on the estimated action values\n","- For epsilon=0.01, the algorithm selects the optimal action reward was 502, indicating that it may not have explored enough to estimate the true action values accurately\n","- For epsilon=0.3, the algorithm selects the optimal action reward was 946, indicating that it may have explored enough to estimate the true action values accurately\n","\n","#### For the UCB method:\n","\n","- The estimated action values improve as the number of rounds increases\n","- The algorithm tends to select the optimal action more often compared to the epsilon-greedy method\n","- For c=1.5, the algorithm selects the optimal action reward was 290, indicating that it may have explored enough to estimate the true action values accurately\n","\n","### The UCB approach had a total reward of 290, which was lower than the total reward for the epsilon-greedy approach with ε=0.3, which was 729. This suggests that the epsilon-greedy approach with ε=0.3 may have performed better than the UCB approach."],"metadata":{"id":"YpSHqFGT2Esk"}}]}