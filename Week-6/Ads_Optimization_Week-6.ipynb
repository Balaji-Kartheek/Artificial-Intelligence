{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1-RNoFrldnILFFOk3JYKJIzBdasTIPUPg","authorship_tag":"ABX9TyOTdS4L+etIShLRBx6BORu3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iGFqtYXm0L0m"},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"markdown","source":["### A. Write down the MAB agent problem formulation in your own words."],"metadata":{"id":"WRP4hCwjFsUj"}},{"cell_type":"markdown","source":["- The MAB agent needs to decide which ad to display at each time step to maximize the total number of clicks received over a large number of time steps.\n","\n"],"metadata":{"id":"zs9De-IpF0Su"}},{"cell_type":"markdown","source":["### B.Compute the total rewards after 2000-time steps using the ε-greedy action. for \n","- ε=0.01, \n","- ε= 0.3"],"metadata":{"id":"exssHhZI1qZz"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","\n","df = pd.read_csv(\"/content/drive/MyDrive/Artificial_Intelligence/Week-6/Ads_Optimisation.csv\")\n","num_ads = 10\n","num_rounds = 2000\n","\n","ads_selected = []\n","total_rewards = 0\n","num_selections = np.zeros(num_ads)\n","sum_rewards = np.zeros(num_ads)\n","\n","epsilons = [0.01, 0.3]\n","\n","for eps in epsilons:\n","    for n in range(num_rounds):\n","        if np.random.random() > eps:   \n","            ad = np.argmax(sum_rewards / (num_selections + 1))    # random ad reward selection\n","        else:\n","            ad = np.random.randint(num_ads)     # random selection\n","        ads_selected.append(ad)\n","        reward = df.values[n, ad]\n","        total_rewards += reward\n","        num_selections[ad] += 1\n","        sum_rewards[ad] += reward\n","        if n % 100 == 0:\n","          print(f\"Selected ad for round {n}: {ad} (Epsilon={eps})\")\n","    \n","    print(\"------------------------------------------------------------------\")\n","    print(f\"Total reward for epsilon={eps}: {total_rewards}\")\n","    max_val = max(ads_selected)\n","    print(f\"Ad selected in UCB method is:= {max_val}\")\n","    print(\"_________________________________________________________________________________________________________________________________\")\n","    \n"],"metadata":{"id":"mA8ZviBo0WpV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### C. Compute the total rewards after 2000-time steps using the Upper-Confidence-Bound action method for c= 1.5"],"metadata":{"id":"3D2W3Xqq1vSM"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","df = pd.read_csv(\"/content/drive/MyDrive/Artificial_Intelligence/Week-6/Ads_Optimisation.csv\")\n","num_ads = 10\n","num_rounds = 2000\n","\n","ads_selected = []\n","total_rewards = 0\n","num_selections = np.zeros(num_ads)\n","sum_rewards = np.zeros(num_ads)\n","\n","# exploration parameter c\n","c = 1.5\n","\n","for n in range(num_rounds):\n","    # select ad using UCB action method\n","    ad = 0\n","    max_upper_bound = 0\n","    for i in range(num_ads):\n","        if num_selections[i] > 0:\n","            average_reward = sum_rewards[i] / num_selections[i]\n","            delta_i = np.sqrt(2 * np.log(n+1) / num_selections[i])\n","            upper_bound = average_reward + c * delta_i\n","        else:\n","            upper_bound = 1e400\n","        if upper_bound > max_upper_bound:\n","            max_upper_bound = upper_bound\n","            ad = i\n","\n","    ads_selected.append(ad)\n","\n","    reward = df.values[n, ad]\n","    total_rewards += reward\n","    num_selections[ad] += 1\n","    sum_rewards[ad] += reward\n","\n","print(f\"Total reward for c={c}: {total_rewards}\")\n","max_val = max(ads_selected)\n","print(f\"Ad selected in UCB method is:= {max_val}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZWVQUFDi1vF8","executionInfo":{"status":"ok","timestamp":1679237220307,"user_tz":-330,"elapsed":5,"user":{"displayName":"Kartheek T","userId":"13688682006331270789"}},"outputId":"69304c67-1020-448e-dfac-f9c1461b533b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total reward for c=1.5: 290\n","Ad selected in UCB method is:= 9\n"]}]},{"cell_type":"markdown","source":["### D. For all approaches, explain how the action value estimated compares to the optimal action"],"metadata":{"id":"vPMvKyF71mrT"}},{"cell_type":"markdown","source":["\n","\n","### The UCB approach had a total reward of 290, which was lower than the total reward for the epsilon-greedy approach with ε=0.3, which was 946. This suggests that the epsilon-greedy approach with ε=0.3 may have performed better than the UCB approach."],"metadata":{"id":"YpSHqFGT2Esk"}}]}